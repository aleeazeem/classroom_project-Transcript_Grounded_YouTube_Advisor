{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00c6b81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d8d18d",
   "metadata": {},
   "source": [
    "Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c475ada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def open_file(file_path: str):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return f.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return pd.DataFrame(columns=[\"id\", \"start_time\", \"end_time\", \"text\"])\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {e}\")\n",
    "        return pd.DataFrame(columns=[\"id\", \"start_time\", \"end_time\", \"text\"])\n",
    "\n",
    "\n",
    "def remove_redundunat_data(transcript):\n",
    "    if transcript.startswith('WEBVTT'):\n",
    "        transcript = transcript.split('\\n', 1)[1] if '\\n' in transcript else ''\n",
    "    \n",
    "    # Split transcript blocks by double newlines\n",
    "    return re.split(r'\\n\\s*\\n', transcript.strip())\n",
    "\n",
    "\n",
    "#data ingestion\n",
    "def convert_webvtt_to_dataframe(blocks):\n",
    "    \"\"\"\n",
    "    Convert WEBVTT transcript file to pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the WEBVTT file\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns [id, start_time, end_time, text]\n",
    "    \"\"\"\n",
    "    \n",
    "    records = []\n",
    "    \n",
    "    for block in blocks:\n",
    "        if not block.strip():\n",
    "            continue\n",
    "            \n",
    "        lines = [line.strip() for line in block.strip().split('\\n') if line.strip()]\n",
    "        \n",
    "        # Skip if not enough lines for a valid block\n",
    "        if len(lines) < 3:\n",
    "            continue\n",
    "            \n",
    "        # Try to parse the first line as ID\n",
    "        try:\n",
    "            idx = int(lines[0])\n",
    "        except ValueError:\n",
    "            continue\n",
    "        \n",
    "        # Second line should be the timestamp\n",
    "        time_range = lines[1]\n",
    "        \n",
    "        # Check if it matches timestamp pattern\n",
    "        if '-->' not in time_range:\n",
    "            continue\n",
    "            \n",
    "        # Rest of the lines are the text\n",
    "        text = \" \".join(lines[2:]).strip()\n",
    "        \n",
    "        # Extract start and end times\n",
    "        try:\n",
    "            start, end = re.split(r'\\s*-->\\s*', time_range)\n",
    "            records.append([idx, start.strip(), end.strip(), text])\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Could not parse timestamp in block {idx}: {time_range}\")\n",
    "            continue\n",
    "    \n",
    "    # Convert to DataFrame - ONLY these 4 columns\n",
    "    return pd.DataFrame(records, columns=[\"id\", \"start_time\", \"end_time\", \"text\"])\n",
    "\n",
    "# Simple usage\n",
    "#if __name__ == \"__main__\":\n",
    "#    df = convert_webvtt_to_dataframe('transcripts/aprilynne.txt')\n",
    "#    print(\"Columns:\", df.columns.tolist())\n",
    "#    print(f\"Shape: {df.shape}\")\n",
    "#    print(\"\\nFirst 5 records:\")\n",
    "#    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebfa5dea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>00:00:03.495</td>\n",
       "      <td>00:00:05.235</td>\n",
       "      <td>Hey, April Lynn, how are you?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>00:00:06.395</td>\n",
       "      <td>00:00:08.275</td>\n",
       "      <td>I am doing well, thank you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>00:00:09.575</td>\n",
       "      <td>00:00:13.635</td>\n",
       "      <td>That's awesome. So, hi everyone. Uh, my name i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>00:00:13.775</td>\n",
       "      <td>00:00:16.035</td>\n",
       "      <td>I'm a creator success manager at Spotter.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>00:00:16.375</td>\n",
       "      <td>00:00:17.755</td>\n",
       "      <td>We are super, super stoked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>1254</td>\n",
       "      <td>01:10:16.065</td>\n",
       "      <td>01:10:17.885</td>\n",
       "      <td>and I hope you were able to get something usef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>1255</td>\n",
       "      <td>01:10:18.525</td>\n",
       "      <td>01:10:21.295</td>\n",
       "      <td>This was so fantastic. Thank you so much for y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>1256</td>\n",
       "      <td>01:10:21.555</td>\n",
       "      <td>01:10:24.415</td>\n",
       "      <td>I'm sure everybody learned so much. So thank you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>1257</td>\n",
       "      <td>01:10:25.485</td>\n",
       "      <td>01:10:26.235</td>\n",
       "      <td>Thank you so much.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>1258</td>\n",
       "      <td>01:10:32.435</td>\n",
       "      <td>01:10:33.175</td>\n",
       "      <td>Thanks, Cody.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1258 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    start_time      end_time  \\\n",
       "0        1  00:00:03.495  00:00:05.235   \n",
       "1        2  00:00:06.395  00:00:08.275   \n",
       "2        3  00:00:09.575  00:00:13.635   \n",
       "3        4  00:00:13.775  00:00:16.035   \n",
       "4        5  00:00:16.375  00:00:17.755   \n",
       "...    ...           ...           ...   \n",
       "1253  1254  01:10:16.065  01:10:17.885   \n",
       "1254  1255  01:10:18.525  01:10:21.295   \n",
       "1255  1256  01:10:21.555  01:10:24.415   \n",
       "1256  1257  01:10:25.485  01:10:26.235   \n",
       "1257  1258  01:10:32.435  01:10:33.175   \n",
       "\n",
       "                                                   text  \n",
       "0                         Hey, April Lynn, how are you?  \n",
       "1                           I am doing well, thank you.  \n",
       "2     That's awesome. So, hi everyone. Uh, my name i...  \n",
       "3             I'm a creator success manager at Spotter.  \n",
       "4                            We are super, super stoked  \n",
       "...                                                 ...  \n",
       "1253  and I hope you were able to get something usef...  \n",
       "1254  This was so fantastic. Thank you so much for y...  \n",
       "1255  I'm sure everybody learned so much. So thank you.  \n",
       "1256                                 Thank you so much.  \n",
       "1257                                      Thanks, Cody.  \n",
       "\n",
       "[1258 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts = open_file('transcripts/aprilynne.txt')\n",
    "blocks = remove_redundunat_data(transcripts)\n",
    "df = convert_webvtt_to_dataframe(blocks)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc7f540",
   "metadata": {},
   "source": [
    "Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a9c60f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values per column:\n",
      "id            0\n",
      "start_time    0\n",
      "end_time      0\n",
      "text          0\n",
      "dtype: int64\n",
      "\n",
      "Empty strings per column:\n",
      "id            0\n",
      "start_time    0\n",
      "end_time      0\n",
      "text          0\n",
      "dtype: int64\n",
      "\n",
      "Combined (null or empty):\n",
      "id            0\n",
      "start_time    0\n",
      "end_time      0\n",
      "text          0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def drop_invalid_text_rows(df, text_column):\n",
    "    \"\"\"\n",
    "    Drop rows where the text column has null, empty, or whitespace-only values.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input DataFrame\n",
    "        text_column (str): Name of the text column to check (default: 'text')\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Cleaned DataFrame\n",
    "    \"\"\"\n",
    "    # Remove rows with null, empty, or whitespace-only text\n",
    "    cleaned_df = df[\n",
    "        df[text_column].notna() & \n",
    "        (df[text_column].str.strip() != '')\n",
    "    ].reset_index(drop=True)\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "df = drop_invalid_text_rows(df, 'text')\n",
    "print(\"Null values per column:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(\"\\nEmpty strings per column:\")\n",
    "print((df == '').sum())\n",
    "\n",
    "print(\"\\nCombined (null or empty):\")\n",
    "print((df.isnull() | (df == '')).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b6ea111",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     id    start_time      end_time  \\\n",
      "0       [1, 2, 3, 4, 5]  00:00:03.495  00:00:17.755   \n",
      "1      [6, 7, 8, 9, 10]  00:00:17.755  00:00:29.195   \n",
      "2  [11, 12, 13, 14, 15]  00:00:29.215  00:00:39.875   \n",
      "3  [16, 17, 18, 19, 20]  00:00:39.935  00:00:53.915   \n",
      "4  [21, 22, 23, 24, 25]  00:00:53.915  00:01:04.835   \n",
      "\n",
      "                                                text  \n",
      "0  Hey, April Lynn, how are you? I am doing well,...  \n",
      "1  creator success manager at Spotter. We are sup...  \n",
      "2  to share it with you guys, um, to introduce he...  \n",
      "3  uh, and YouTube strategist who's built a name ...  \n",
      "4  Um, and today she's gonna be taking us through...  \n",
      "Hey, April Lynn, how are you? I am doing well, thank you. That's awesome. So, hi everyone. Uh, my name is Sie. I'm a creator success manager at Spotter. We are super, super stoked\n",
      "creator success manager at Spotter. We are super, super stoked to have April Lynn on today's webinar to teach us how to make a killer intro. So a little bit about April Lynn, I'll just, I wrote a little bio about her, so I wanted to share it with you guys, um, to introduce her.\n"
     ]
    }
   ],
   "source": [
    "def group_rows_into_splitters(df, group_size=5):\n",
    "    grouped_data = []\n",
    "    \n",
    "    for i in range(0, len(df), group_size):\n",
    "        group = df.iloc[i:i+group_size]\n",
    "        \n",
    "        if len(group) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Get current group's text\n",
    "        current_text = ' '.join(group['text'].tolist())\n",
    "        \n",
    "        # Get last 10 words from previous group (if exists)\n",
    "        if i > 0:  # Not the first group\n",
    "            prev_group_text = grouped_data[-1]['text']  # Previous group's text\n",
    "            prev_words = prev_group_text.split()\n",
    "            last_10_words = ' '.join(prev_words[-10:]) if len(prev_words) >= 10 else prev_group_text\n",
    "            \n",
    "            # Combine: last 10 words from previous + current text\n",
    "            combined_text = last_10_words + ' ' + current_text\n",
    "        else:\n",
    "            # First group - no previous text to add\n",
    "            combined_text = current_text\n",
    "            \n",
    "        grouped_row = {\n",
    "            'id': group['id'].tolist(),                # [1,2,3,4,5]\n",
    "            'start_time': group['start_time'].iloc[0], # First start_time\n",
    "            'end_time': group['end_time'].iloc[-1],    # Last end_time  \n",
    "            'text': combined_text                      # Combined text with overlap\n",
    "        }\n",
    "        \n",
    "        grouped_data.append(grouped_row)\n",
    "    \n",
    "    return pd.DataFrame(grouped_data)\n",
    "\n",
    "df_grouped = group_rows_into_splitters(df, group_size=5)\n",
    "print(df_grouped.head())\n",
    "print(df_grouped['text'][0])\n",
    "print(df_grouped['text'][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f001119e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     id    start_time      end_time  \\\n",
      "0       [1, 2, 3, 4, 5]  00:00:03.495  00:00:17.755   \n",
      "1      [6, 7, 8, 9, 10]  00:00:17.755  00:00:29.195   \n",
      "2  [11, 12, 13, 14, 15]  00:00:29.215  00:00:39.875   \n",
      "3  [16, 17, 18, 19, 20]  00:00:39.935  00:00:53.915   \n",
      "4  [21, 22, 23, 24, 25]  00:00:53.915  00:01:04.835   \n",
      "\n",
      "                                                text  \\\n",
      "0  Hey, April Lynn, how are you? I am doing well,...   \n",
      "1  creator success manager at Spotter. We are sup...   \n",
      "2  to share it with you guys, um, to introduce he...   \n",
      "3  uh, and YouTube strategist who's built a name ...   \n",
      "4  Um, and today she's gonna be taking us through...   \n",
      "\n",
      "                                          clean_text  word_count  \n",
      "0  hey, april lynn, how are you? i am doing well,...          34  \n",
      "1  creator success manager at spotter. we are sup...          55  \n",
      "2  to share it with you guys, um, to introduce he...          50  \n",
      "3  uh, and youtube strategist who's built a name ...          59  \n",
      "4  um, and today she's gonna be taking us through...          39  \n",
      "think you guys have gotten a lot of snow recently. Um, two months, um, and a scale of two when it comes to confidence. Well, welcome to YouTube. So glad to have you and hopefully we can get that number a little bit higher\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Download required data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "df_processed = df_grouped.copy()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    \n",
    "    try:\n",
    "        # Lowercase & tokenize\n",
    "        tokens = word_tokenize(str(text).lower())\n",
    "        \n",
    "        clean_tokens = []\n",
    "        for word in tokens:\n",
    "            # Remove punctuation & non-alphabetic with regex\n",
    "            word = re.sub(r'[^a-z]', '', word)\n",
    "            \n",
    "            if word and word not in stop_words:\n",
    "                clean_tokens.append(word)\n",
    "                \n",
    "        return ' '.join(clean_tokens)\n",
    "    except LookupError:\n",
    "        return str(text).lower()\n",
    "\n",
    "# Add processed columns\n",
    "df_processed['clean_text'] = df_processed['text'].apply(clean_text)\n",
    "df_processed['word_count'] = df_processed['text'].apply(\n",
    "    lambda x: len(str(x).split()) if pd.notna(x) else 0\n",
    ")\n",
    "print(df_processed.head())\n",
    "\n",
    "print(df_processed['text'][10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdb5f5f",
   "metadata": {},
   "source": [
    "Create embeddings and vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4dc29d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# %pip install chromadb\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "332845e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 252 records into Chroma.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load your DataFrame (assuming df is already created)\n",
    "# df has: id | start_time | end_time | text | clean_text\n",
    "\n",
    "# 2. Initialize Chroma client\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")  # creates/loads DB on disk\n",
    "\n",
    "# 3. Create or get a collection\n",
    "collection = client.get_or_create_collection(name=\"video_transcripts\")\n",
    "\n",
    "# 4. Load SentenceTransformer\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')  # lightweight, good for transcripts\n",
    "\n",
    "# 5. Generate embeddings and insert into Chroma\n",
    "for _, row in df_processed.iterrows():\n",
    "    embedding = model.encode(row[\"clean_text\"], convert_to_numpy=True)\n",
    "    #embedding = model.encode(row[\"clean_text\"]).tolist()  # convert to Python list\n",
    "\n",
    "    metadata = {\n",
    "        \"id\": str(row[\"id\"]),\n",
    "        \"start_time\": row[\"start_time\"],\n",
    "        \"end_time\": row[\"end_time\"],\n",
    "        \"text\": row[\"text\"]\n",
    "    }\n",
    "\n",
    "    collection.add(\n",
    "        ids=[str(row[\"id\"])],                  # must be string\n",
    "        embeddings=[embedding],                # embedding vector\n",
    "        documents=[row[\"clean_text\"]],         # store clean text\n",
    "        metadatas=[metadata]                   # store metadata\n",
    "    )\n",
    "\n",
    "print(f\"Inserted {df_processed.shape[0]} records into Chroma.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb3a78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rounded start_time: 00:47:14\n",
      "Rounded end_time: 00:47:15\n",
      "Retrieved Text: uh, and youtube strategist who's built a name for herself by breaking down what actually works on youtube. so you guys are in for a treat today. so she coins herself as just a girl who loves youtube, but we all know that you're more than that april lyn. um, and today she's gonna be taking us through how\n",
      "Start Time: 00:00:39\n",
      "Start Time: 00:00:54\n",
      "---\n",
      "Retrieved Text: creator success manager at spotter. we are super, super stoked to have april lynn on today's webinar to teach us how to make a killer intro. so a little bit about april lynn, i'll just, i wrote a little bio about her, so i wanted to share it with you guys, um, to introduce her.\n",
      "Start Time: 00:00:17\n",
      "Start Time: 00:00:30\n",
      "---\n",
      "Retrieved Text: to share it with you guys, um, to introduce her. and then i'll pass it off to april lynn to take us through what she has for us today. but i'm excited to introduce april lynn, an amazing content creator, uh, and youtube strategist who's built a name for herself\n",
      "Start Time: 00:00:29\n",
      "Start Time: 00:00:40\n",
      "---\n",
      "Retrieved Text: you just thought, eh, i'm just gonna sit back, relax, listen to april and talk, no, i'm going to make you work. uh, but we're gonna have a fun time doing it and we're going to learn a lot along the way. and it all starts with the first five seconds of your video.\n",
      "Start Time: 00:05:58\n",
      "Start Time: 00:06:12\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "query = \"What April Lyn is talking about?\"\n",
    "results = collection.query(\n",
    "    query_texts=[query],\n",
    "    n_results=4\n",
    ")\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "\n",
    "def round_time_string(time_str: str, kind: str = \"start\") -> str:\n",
    "    \"\"\"\n",
    "    Round a time string (HH:MM:SS.sss) to floor (start) or ceil (end).\n",
    "\n",
    "    Args:\n",
    "        time_str (str): Timestamp like '00:47:14.185'\n",
    "        kind (str): 'start' -> floor, 'end' -> ceil\n",
    "\n",
    "    Returns:\n",
    "        str: Rounded time string in HH:MM:SS format\n",
    "    \"\"\"\n",
    "    # Parse time string\n",
    "    dt = datetime.strptime(time_str, \"%H:%M:%S.%f\")\n",
    "\n",
    "    # Total seconds since midnight\n",
    "    total_seconds = dt.hour * 3600 + dt.minute * 60 + dt.second + dt.microsecond / 1e6\n",
    "\n",
    "    if kind == \"start\":\n",
    "        rounded_seconds = math.floor(total_seconds)\n",
    "    elif kind == \"end\":\n",
    "        rounded_seconds = math.ceil(total_seconds)\n",
    "    else:\n",
    "        raise ValueError(\"kind must be 'start' or 'end'\")\n",
    "\n",
    "    # Convert back to HH:MM:SS\n",
    "    rounded_time = str(timedelta(seconds=rounded_seconds))\n",
    "\n",
    "    # Force HH:MM:SS format with zero padding\n",
    "    return str(datetime.strptime(rounded_time, \"%H:%M:%S\").time())\n",
    "\n",
    "# ✅ Example usage\n",
    "start_time = \"00:47:14.885\"\n",
    "end_time   = \"00:47:14.185\"\n",
    "rounded_start = round_time_string(start_time, kind=\"start\")\n",
    "rounded_end   = round_time_string(end_time, kind=\"end\")\n",
    "print(\"Rounded start_time:\", rounded_start)  # 00:47:14\n",
    "print(\"Rounded end_time:\", rounded_end)      # 00:47:15\n",
    "\n",
    "\n",
    "for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "    print(\"Retrieved Text:\", doc)\n",
    "    print(\"Start Time:\", round_time_string(meta['start_time'], 'start'))\n",
    "    print(\"Start Time:\", round_time_string(meta['end_time'], 'end'))\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv (3.10.18)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
